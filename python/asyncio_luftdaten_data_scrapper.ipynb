{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrdqkEMTW_Ty"
   },
   "source": [
    "This scripts retrieve sensors data from the http://archive.luftdaten.info/ \n",
    "It uses as input a list of .csv sensors files url and save the resulting data as .csv\n",
    "\n",
    "**this sript does not work on windows pc because of asyncio**, in this case, you can upload this notebook on google colab.\n",
    "The asyncio library allows to run in this case 20 threads in parallel, making it much faster to scrap the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqTE7tBDRHot"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import lxml.etree\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import io\n",
    "import sqlalchemy\n",
    "from aiohttp import ClientSession,TCPConnector\n",
    "import nest_asyncio\n",
    "\n",
    "import config\n",
    "\n",
    "# asyncio and jupyter cause trouble, this is a fix :\n",
    "# https://markhneedham.com/blog/2019/05/10/jupyter-runtimeerror-this-event-loop-is-already-running/\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1\n"
     ]
    }
   ],
   "source": [
    "## beautiful asyncio needs beautiful python 3.7\n",
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "2EZiTV8Z883i",
    "outputId": "409df95a-d43b-4de5-fa76-7c6f83638201"
   },
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine(f\"sqlite:///{config.DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the list of lufdaten sensors (see *luftdaten_geo.ipynb* to get this list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sensors = pd.read_sql('sensors_BB', engine)\n",
    "df_sensors['endpoint_url']= df_sensors.sensor_type_name+\"_sensor_\"+df_sensors.sensor_id.astype(str)+\".csv\"\n",
    "df_sensors['endpoint_url']= df_sensors['endpoint_url'].str.lower()\n",
    "df_sensors.sensor_type_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we set the date range to scrap data (luftdaten data start from 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LeUUb_cy9Ph5"
   },
   "outputs": [],
   "source": [
    "def set_dt_range(start='1/1/2017'):\n",
    "    dt_range = pd.date_range(start, end=datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    dt_range = dt_range.strftime(\"%Y-%m-%d\")\n",
    "    dt_range = dt_range.tolist()\n",
    "    return dt_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L9bYBKfp9iE1"
   },
   "outputs": [],
   "source": [
    "def clean_data(content):\n",
    "   \n",
    "    df_data = pd.read_csv(io.StringIO(content),sep=';')\n",
    "  \n",
    "    sensor_type = df_data.sensor_type.head(1).values\n",
    "    \n",
    "    # we tidy the data because we are cool people :\n",
    "    if sensor_type  in ['DHT22', 'BME280', 'HTU21D', 'BMP180', 'BMP280']:\n",
    "        df_data = pd.melt(df_data, id_vars=['sensor_id','timestamp'], value_vars=['temperature','humidity'])\n",
    "    if sensor_type==\"SDS011\":\n",
    "        df_data = pd.melt(df_data, id_vars=['sensor_id','timestamp'], value_vars=['P1','P2'])\n",
    "        \n",
    "    return df_data\n",
    "\n",
    "def aggregate_data(df_data):\n",
    "    try:\n",
    "        # aggregate data per hour\n",
    "        df_data['timestamp'] = pd.to_datetime(df_data['timestamp'])\n",
    "        df_data['datehour']= df_data['timestamp'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,0))\n",
    "        df_data = df_data.groupby(['variable','datehour']).mean()\n",
    "\n",
    "        return df_data\n",
    "    except Exception as e:\n",
    "        print(url)\n",
    "        print(e)\n",
    "        return None    \n",
    "  \n",
    "regex = re.compile(\"^.*?\\.(zip|csv)$\")\n",
    "\n",
    "\n",
    "def get_list_of_files(dt):\n",
    "    url = f'http://archive.luftdaten.info/{dt}'\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        tree = html.fromstring(r.content)\n",
    "        files = tree.xpath('//a/@href')\n",
    "        return list(filter(regex.search, files))\n",
    "    except Exception as e :\n",
    "        print(e)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(url, session):\n",
    "    async with session.get(url) as response:\n",
    "        content = await response.text()\n",
    "        df_data = clean_data(content)\n",
    "        \n",
    "        return aggregate_data(df_data)\n",
    "\n",
    "async def run(lista):\n",
    "   \n",
    "    tasks = []\n",
    "\n",
    "    # Fetch all responses within one Client session,\n",
    "    # keep connection alive for all requests.\n",
    "    # we set the connector to 10 because we are well educated people who play by the rules :\n",
    "    connector = TCPConnector(limit=10)\n",
    "    async with ClientSession(connector=connector) as session:\n",
    "        for url in lista:\n",
    "            task = asyncio.ensure_future(fetch(url, session))\n",
    "            tasks.append(task)\n",
    "\n",
    "        responses = await asyncio.gather(*tasks,return_exceptions=True )\n",
    "        # you now have all response bodies in this variable\n",
    "        return (responses)\n",
    "    \n",
    "def scrap_all_data_one_day(dt):\n",
    "    \n",
    "    lista = get_list_of_files(dt)\n",
    "\n",
    "    sensors_available= [l.split('_', 1)[-1] for l in lista]\n",
    "    sensors_to_retrieve = list(set(sensors_available) & set(all_sensors_BB))\n",
    "\n",
    "    print(f'{dt}:{len(sensors_to_retrieve)} files to retrieve')\n",
    "\n",
    "    all_url_of_date = [ROOT_URL.format(dt,dt,l) for l in sensors_to_retrieve]\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    future = asyncio.ensure_future(run(all_url_of_date))\n",
    "    data = loop.run_until_complete(future)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for d in data:\n",
    "        df = pd.concat([df,d],axis=0)\n",
    "\n",
    "    return df.reset_index()\n",
    "\n",
    "def scrap_all_data():\n",
    "    df_master = pd.DataFrame()\n",
    "\n",
    "    for i,dt in enumerate(date_range):\n",
    "      try:\n",
    "        df = scrap_all_data_one_day(dt)\n",
    "        df_master=pd.concat([df_master,df],axis=0)\n",
    "\n",
    "        if i%100==0:\n",
    "            df_master.to_csv(f'master_{str(i)}.csv')\n",
    "\n",
    "      except Exception as e:\n",
    "        print(f'error for {dt}:')\n",
    "        print(e)\n",
    "\n",
    "    df_master.to_csv('master_all_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = \"http://archive.luftdaten.info/{}/{}_{}\"\n",
    "date_range = set_dt_range()\n",
    "all_sensors_BB = df_sensors.endpoint_url.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/master_all.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stations\n",
      "data_CO\n",
      "data_NO2\n",
      "data_O3\n",
      "data_PM10\n",
      "data_SO2\n",
      "data_CO_1TMW\n",
      "data_NO2_1TMW\n",
      "sensors_BB\n",
      "sensors_BB_data\n"
     ]
    }
   ],
   "source": [
    "res = engine.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "\n",
    "for r in res:\n",
    "    print(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10 ** 6\n",
    "#for chunk in pd.read_csv('../data/master_all.csv', chunksize=chunksize):\n",
    "    #chunk.to_sql(name='sensors_BB_data', con=engine,if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn=sqlite3.connect('../data/dwd.db')\n",
    "conn.execute(\"VACUUM\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db = pd.read_sql('sensors_BB_data', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_db.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "asyncio_scrapper.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
